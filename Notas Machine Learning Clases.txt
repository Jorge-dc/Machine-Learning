----------------
MACHINE LEARNING
----------------
neurona <- algo que contiene un numero
regularmente se entrena con el 70% de los datos y se prueba con el 30%

#HERRAMIENTAS COMPUTACIONALES
1. Big Data:
	- Hadoop
		framework open source para almacenar datos y ejecutar aplicaciones en clusters de computadores
		almacenamiento masivo, procesamiento y capacidad de manejar tareas de forma practicamente ilimitada

	- Amazon Web Services
		proveedor de servicios de almacenamiento, aplicaciones moviles, bases de datos y otros en la nube

	- Apache Spark
		primer software open source con programacion distribuida (el trabajo se distribuye en clusters que trabajan como uno)
		se pueden programar aplicaciones usando diferentes lenguajes como Java, Scala, Python o R

2. Data Mining:
	- R (estadistico)
	- Python (informatico)

3. Visualización (Business Intelligence):
	- Power BI
	- Tableau	

#TIPOS DE APRENDIZAJE
1. Aprendizaje Supervisado:
	se le administran las variables para que la maquina aprenda a predecir un resultado
	el modelo aprende a partir de las variables explicativas asociadas a la variable respuesta (y)
	regresion lineal, vecino mas cercano, arbol de decision

2. Aprendizaje No Supervisado: 
	la maquina por si sola genera grupos dentro del set de datos
	no hay variable respuesta (y) asociada
	k-means, k-medians, sistemas de recomendacion

3. Aprendizaje de Refuerzo: 
	se aplica cuando hay pocos datos
	funciona con simulaciones de ensayo y error
	recompensa y castigo

#FALENCIAS DEL MODELO
1. Underfitting:
	subajuste, alto sesgo y baja varianza
	subestmiacion del modelo, no es capaz de encontrar una representacion adecuada de los datos durante la clasificacion
	se necesita aumentar la complejidad del modelo (mas datos, entender otra variable)
	pocos datos de entrenamiento
	aumentar el numero de covariables 
	eliminar el ruido de los datos
	aumentar la etapa de entrenamiento

2. Overfitting:
	al entrenar demasiado o con datos raros, el modelo se ajusta a caracteristicas muy especificas de los datos de entrenamiento
	sobreestimacion del modelo, los datos nuevos quedan mal clasificados ya que el modelo aprendio demasiado bien los datos de entrenamiento
	se necesita reducir la complejidad del modelo
	utilizar metodos de regularizacion (Ridge o Lasso)
	disminuir la etapa de entrenamiento

------------------
VALIDACION CRUZADA
------------------
#K-FOLD CROSS VALIDATION
los datos de entrenamiento se dividen en k subconjuntos de entrenamiento y validacion
consiste en dividir los datos en k subconjuntos de igual tamaño y repetir el procedimiento k veces tal que cada k subconjunto sea
el conjunto de prueba y los otros k-1 subconjuntos forman el conjunto de entrenamiento

	- Caso regresion:
		RMSE: raiz del error cuadratico medio
		MAE: error absoluto medio (tambien puede ser mediano)
		R2: coeficiente de determinacion

	- Caso clasificacion:
		Accuracy: exactitud
		K: estadistico Kappa, indica la probabilidad de acertar al azar	

	En R:
Modelo de regresion lineal
	library(caret)
	train_control_cv <- trainControl(method = "cv", number = 10)
	model_cv <- train(speed ~., data = cars, method = "lm",		<- method se puede cambiar a "glm"
	trControl = train_control_cv)
	model_cv$results									<- resultados resumidos
	model_cv$resample									<- si las metricas son muy distintas se recomienda cambiar los datos

Modelo de clasificacion
	load("ENS_completa.RData")
	ENS$diabetes <- as.factor(ENS$diabetes)
	train_control_cv <- trainControl(method = "cv", number = 10)
	model_cv <- train(diabetes ~., data = ENS, method = "glm",
		trControl = train_control_cv)
	model_cv$results
	model_cv$resample

	En Python:
Modelo de regresion lineal
	from sklearn.model_selection import cross_validate
	from sklearn.model_selection import KFold
	from sklearn import linear_model
	from sklearn import datasets
	X, y = datasets.load_diabetes(return_X_y=True)
	kf = KFold(n_splits=10)
	lr = linear_model.LinearRegression()
	lr.fit(X, y)
	lr_cv = cross_validate(lr, X, y, cv=kf,				<- en cada iteracion se calcula el R2 y error cuadratico medio
	scoring=('r2', 'neg_mean_squared_error'))
	print("Metricas cross_validation", lr_cv['test_score'])

Modelo de clasificacion
	from sklearn.model_selection import cross_validate
	from sklearn.model_selection import KFold
	from sklearn.linear_model import LogisticRegression
	from sklearn import datasets
	cancer = datasets.load_breast_cancer()
	X , y = cancer.data, cancer.target
	kf = KFold(n_splits=10)
	logic = LogisticRegression()
	logic.fit(X, y)
	logic_cv = cross_validate(logic, X, y, cv=kf, scoring = "accuracy")	<- para cada set de iteracion se calcula acurracy
	print("Metricas cross_validation", logic_cv['test_score'])

#LEAVE ONE OUT CROSS VALIDATION (LOOCV)
en cada iteracion se utiliza una sola obs para los datos de prueba, dejando al resto como set entrenamiento
el modelo se ajusta utilizando n-1 obs de entrenamiento

	En R:
Modelo de regresion lineal
	train_control_lpocv <- trainControl(method = "LOOCV")
	model_lpocv <- train(speed ~., data = cars, method = "lm",
	trControl = train_control_lpocv)
	model_lpocv$results

Modelo de clasificacion
	train_control_lpocv <- trainControl(method = "LOOCV")
	model_lpocv <- train(diabetes ~., data = ENS, method = "glm",
		trControl = train_control_lpocv)
	model_lpocv$results

	En Python:
Modelo de regresion lineal
	from sklearn.model_selection import LeaveOneOut
	X, y = datasets.load_diabetes(return_X_y=True)
	loo = LeaveOneOut()
	lr = linear_model.LinearRegression()
	lr.fit(X, y)
	lr_loo = cross_validate(lr, X, y, cv=loo, scoring=('neg_mean_squared_error'))
	print("Media de cross_validation:", logic_loo['test_score'].mean())

Modelo de clasificacion
	from sklearn.model_selection import LeaveOneOut
	X , y = cancer.data, cancer.target
	loo = LeaveOneOut()
	logic = LogisticRegression()
	logic.fit(X, y)
	logic_loo = cross_validate(logic, X, y, cv=loo, scoring = "accuracy")
	print("Media de cross_validation:", logic_loo['test_score'].mean())

-----------------------------------------------------
NAIVE BAYES O BAYES INGENUO (MODELO DE CLASIFICACION)
-----------------------------------------------------
es de tipo supervisado, se basa en el teorema de bayes para predecir las probabilidades de pertenecer a distintas clases (modelo de clasificacion)
asume independencia condicional en las caracteristicas
es util para conjuntos de datos muy grandes y en text analytics
ventajas: aunque no es exacto en general toma buenas decisiones, tiene alta velocidad en grandes bases de datos
desventajas: no le va bien cuando hay muchos datos y pocos predictores, dado el supuesto de independencia el modelo no reflejara como son los datos realmente

Procedimiento:
	1. Convertir el conjunto de datos en una tabla de frecuencia
	2. Crear una tabla de probabilidad marginal para cada uno de los casos posibles
	3. Usar Bayes para obtener la probabilidad posterior de cada clase
	4. La clase con la probabilidad posterior mas alta es el resultado de la prediccion

En R:
	library(e1071)	<- modelo de Naive Bayes
	id <- sample(1:nrow(iris), size = 0.7*nrow(iris))
	iris_train <- iris[id,]
	iris_test <- iris[-id,]
	mod_nb <- naiveBayes(Species ~ ., data = iris_train)
	pred_nb <- predict(mod_nb, iris_test)
	table(pred_nb, iris_test$Species)

En Python:
	import numpy as np
	import pandas as pd
	from sklearn import datasets
	data = datasets.load_iris()
	X = pd.DataFrame(data['data'], columns=data['feature_names'])
	y = data['target']
	from sklearn.model_selection import train_test_split
	X_train, X_test, y_train, y_test = train_test_split(X, y,
	test_size = 0.3, random_state = 0)
	from sklearn.naive_bayes import MultinomialNB
	model=MultinomialNB(fit_prior=False).fit(X_train, y_train)		<- ajustamos el modelo
	y_predicted = model.predict(X_test)		<- predicciones en el set de prueba
	pd.crosstab(y_test, y_hat)	<- matriz de confusion

------------------------------------------------------
SUPPORT VECTOR MACHINE O MAQUINAS DE SOPORTE VECTORIAL
------------------------------------------------------
es de tipo supervisado, se usa para modelos de clasificacion pero tambien puede usarse para resolver un problema de regresion
buscar encontrar el hiperplano que mejor separa dos categorias, maximizando el margen de separacion entre ellas
el margen de separacion se define como la distancia perpendicular entre el hiperplano y los registros mas cercanos a cada uno de sus lados
la posicion del hiperplano se define por un conjunto de datos de entrenamientos, estos registros o vectores dan el soporte vectorial
en los casos no lineales (datos discretos), los kernels son funciones que convierten un problema de clasificacion lineal a un espacio dimensional mayor
ventajas: eficaz en espacios de grandes dimensiones y es versatil al poder variar en el kernel (aunque sensible al kernel)
desventajas: dificil interpretacion de los coeficientes y requiere mas tiempo de procesamiento, lo cual puede ser problematico si hay muchos datos

En R:
	library(e1071)
	id <- sample(1:nrow(iris), size = 0.7*nrow(iris))
	iris$Species <- as.factor(iris$Species)
	iris_train <- iris[id,]
	iris_test <- iris[-id,]
	modelo_svm <- svm(formula=Species ~., data = iris_train, kernel='linear', cost=10, scale=FALSE) <- se usa kernel='linear' con modelos lineales
	pred_svm <- predict(object = modelo_svm, newdata = iris_test)
	table(pred_svm, iris_test$Species)

En Python:
	from sklearn import datasets
	cancer = datasets.load_breast_cancer()
	from sklearn.model_selection import train_test_split
	X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.3,random_state=0)
	from sklearn import svm
	svm_linear = svm.SVC(kernel='linear')
	svm_linear.fit(X_train, y_train)
	y_predicted = svm_linear.predict(X_test)
	from sklearn import metrics #métricas
	print("Accuracy:", metrics.accuracy_score(y_test, y_predicted))
	print("Precision:", metrics.precision_score(y_test, y_predicted))
	print("Recall:", metrics.recall_score(y_test, y_predicted))

-------------------
ARBOLES DE DECISION
-------------------
es un modelo con aprendizaje de tipo supervisado
si la variable respuesta es categorica el modelo es de clasificacion, si es numerica entonces el problema es de regresion
en general funciona mejor para problemas de clasificacion (ejemplo de fuga de clientes o desercion de estudiantes)
	- decision node <- nodo de decision, representa una pregunta sobre un atributo
	- root node <- nodo raiz, corresponde a la primera pregunta
	- cada rama representa una posible respuesta a una pregunta
	- leaf node <- hoja o nodo terminal, representa la prediccion (o clase) al seguir ese camino

#CRITERIOS DE RAMIFICACION
	- Entropia o ganancia de informacion <- representa cuanta informacion se gana particionando por cierto atributo (impureza del nodo)
					     <- se busca disminuir la entropia para tener certeza
	- Impureza de Gini <- representa la probabilidad de que la muestra de un nodo se clasifique mal de acuerdo con la distribucion de muestras
				 <- se busca minizar la impureza de Gini

	En R:
ind_train <- partition(1:nrow(datraframe), p = c(0.7, 0.3))
train <- dataframe[ind_train$`1`, ]
test <- dataframe[ind_train$`2`, ]

library(rpart); library(rpart.plot)
tree_entropy <- rpart(variable_y ~ ., data = train,
						  parms = list(split = 'information'),
						  method = 'class')
rpart.plot(tree_entropy)
printcp(tree_gini)	<- para ver 

	En Python:
from sklearn import datasets
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree

X, y = datasets.load_wine(return_X_y = True)
X_train, X_test, y_Train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 2022)

	Metodo de Gini con Python:
tree_gini = DecisionTreeClassifier(criterion = 'gini').fit(X_train, y_train)
fig, ax = plt.subplot(figsize = (12, 12))
plot_tree(tree_gini, filled = True, class_names = ['0', '1', '2'])

	Ventajas:
construccion facil, barata y visual
rapido en clasificar registros nuevos
facil de interpretar
excluye caracteristicas sin importancia

	Desventajas:
facil de sobreajustar
muy sensible ante cambios del set de entrenamiento
dificil de interpretar (arboles grandes)
los modelos pueden inclinarse hacia divisiones de atributos con gran cantidad de niveles

	Variantes:
- Podar el arbol
- Bagging
- Definir profundidad maxima, cantidad de features, etc
- Bosques de arboles (Random Forest)

#PODAR EL ARBOL
se usa el parametro de complejidad para eliminar ramificaciones que pueden generar sobreajuste
la poda ayuda a que el modelo no memorice y no se sobreajuste

	En R:
printcp(tree_gini)
plotcp(tree_gini)	<- se debe determinar desde que tamaño del arbol el error relativo ya no cambia significativamente (cp)
tree_gini_prune <- rpart(variable_y ~., 			<- definimos el costo de complejidad como parametro de control del arbol
				 data = train, 
				 parms = list(split = 'gini', 
					method = 'class', 
					control = rpart.control(cp = valor_obtenido))
rpart.plot(tree_gini_prune)	<- arbol podado

	En Python:
param_grid = {'ccp_alpha': np.linspace(0, 0.5, 10),	<- la poda se puede medir en base a la complejidad o el numero maximo de progundidad
		  'max_depth': bp.arrange(2, 7)}		<- con validacion cruzada se prueban diversos valroes de ambos parametros en una grilla
from sklearn.model_selection import GridSearchCV	   y se elige el que tenga mejor desempeño en el proceso de entrenamiento.	
grid = GridSearchCV(
	estimator = DecisionTreeClassifier(
		max_depth = None, random_state = 2022, criterion = 'gini'),
	para_grid = param_grid,
	scoring ) 'accuracy',
	cv = 10,
	refit = True,
	return_train_score = True)
grid.fit(X_train, y_train)
grid.best_params_		<- identifica los hiperparametros que lograron mejor desempeño
modelo_final = grid.best_estimator_
fig, ax = plt.subplots(figsize = (12, 12))	<- nuevo arbol podado
plot_tree(modelo_final, filled = True, class_names = ['0', '1', '2'])

#BAGGING
el bootstrap es un metodo de remuestreo que consiste en calcular un estadistico con reemplazo y replicando el proceso muchas veces
el bagging o boostrap aggregation es un metodo de ensamblaje que ejecuta muchos modelos con remuestreo para obtener predicciones mas robustas
se le pasan muchos sets de entrenamientos al modelo y compiten en el paso de agregacion
en regresiones compiten por media, en clasificadores compiten por clase mayoritaria
el bagging ayuda a que el modelo no sea tan sensible al conjunto de entrenamiento

#RANDOM FOREST (BOSQUE DE ARBOLES)
se basa en un conjunto de arboles seleccionando aleatoriamente submuestras con reemplazo para elaborar el arbol
esto se hace para mejorar la tasa de clasificacion correcta
es de utilidad cuando tenemos muchas variables
en el bagging tomo las mismas variables (un unico arbol) con distinta data
random forest elige conjuntos random de variables para correr distintos arboles de decision
importancia por permutacion <- identifica la influencia de un predictor sobre una metrica del modelo
					 si el predictor permutado contribuye al modelo, este aumenta su error ya que pierde la informacion de esa variable
impureza de nodos <- cuantifica el incremento total en la pureza de los nodos debido a las divisiones en las que participa el predictor (promedios)

	En R:
library(randomForest)
set.seed(2022)
RanForClas <- randomForest(variable_y ~ ., data = train, ntree= 100, importante = T)
barplot(RanForClas$importance[ , 'MeanDecreaseAccuracy'], names.arg = names(RanForClas$importance[ , 'MeanDecreaseAccuracy'']))

	En Python:
from sklearn.ensemble import RandomForestClassifier
RanForClas = RandomForestClassifier(random_state = 2022, n_estimators = 100).fit(X_train, y_Train)
importancia_pred = pd.DataFrame(
	{'predictor': datasets.load_wine().feature_names,
	'importancia': modelo_final.feature_importance_}
).sort_values('importancia', ascending = False
).reset_index(drop = True)

import seaborn as sns
sns.barplot(x = 'importancia', y = 'predictor', data = importancia_pred)

	Ventajas:
puede manejar miles de variables de entrada y reconocer las significativas
una de las salidas del modelo es el feature importance
es posible usarlo en otros casos (clustering y deteccion de anomalias)

	Desventajas:
perdida de interpretacion
poco control en lo que hace (caja negra)
es mejor para problemas de clasificacion que de regresion

---------------------------------------------------------
ALGORITMOS DE SEGMENTACION: K-MEANS, K-MODES, K-PROTOTYPE
---------------------------------------------------------
son algoritmos no supervisados
	- problemas de agrupacion: establecer clusters o grupos
	- problemas de asociacion: descubrir reglas entre variables que tengan cierta asociacion
1. K-Means: se utiliza cuando los datos corresponden a una matriz numerica
se calcula a partir de la distancia intra clusters (within cluster sums of squares - wss) y distancia entre clusters (between cluster sums of squares - bss)
	la primera indica que tan separados estan los elementos del mismo grupo y que tan lejos estan de su centroide, grupos mas densos (wss pequeño) son mas similares
	la segunda indica que tan lejos estan los grupos entre si, si estan distantes hay mas aislamiento y mayor prob de clasificar de forma precisa
para encontrar el k optimo se usa el grafico del codo tal que posterior a ese punto no se observan drasticas caidas o subidas
2. K-Modes: se utiliza cuando los datos corresponden a un dataframe de atributos categoricos
se parte con k modas (centroides) aleatorias y luego se calculan las distancias (disimilitud) entre el centroide y cada elemento, se define por no coincidencias
3. K-Prototype: se utiliza en datos mixtos, con variables numericas y categoricas

----------------------------
REDUCCION DE DIMENSIONALIDAD
----------------------------
tener muchas variables genera multicolinealidad, empeora el rendimiento del modelo, tiempo alto de procesamiento y no todas las variables aportan info al modelo

#ANALISIS DE COMPONENTES PRINCIPALES (PCA)
busca explicar la mayor parte de variabilidad de los datos con un numero menor de variables que el conjunto de datos original
las pca son una combinacion lineal de las variables y son independientes entre si
es necesario estandarizar las variables (StandardScaler())
requiere solo variables numericas
podemos generar pocas componentes principales que expliquen la amyor cantidad de varianza
	para esto es necesario que las variables tengan aprox la misma escala y varianza
pca se aplica solo a los predictores
un criterio de seleccion de componentes es elegir aquellas con valor propio (eigenvalue) mayor a 1

#ANALISIS FACTORIAL
genera un nuevo conjunto de variables menor al original que expresan lo comun que hay entre ellas
supuestos: los factores son independientes, con media cero y varianza uno
AFE: analisis factorial exploratorio
	busca descubrir la estructura interna de las variables a traves de los factores
AFC: analisis factorial confirmatorio
	determina si el numero de factores obtenidos y sus cargas corresponden a la teoria previa de los datos, arroja un nivel de confianza

PCA:
	explica la cantidad maxima de varianza
	componentes ortogonales entre si
	cada componente es una combinacion lineal de las variables originales
	los componentes no son interpretables
FA:
	explica la covarianza en los datos
	no requiere que los factores sean ortogonales
	las variables originales son combinaciones lineales de las variables latentes (no observadas)
	los factores subyacentes son etiquetables e interpretables

----------------------
DETECCION DE ANOMALIAS
----------------------
una anomalia es una desviacion fuerte de la media de los datos y pueden afectar la estimacion del modelo
se puede utilizar PCA para detectar anomalias
Isolation Forest: tecnica de random forest para detectar anomalias, acepta variables numericas y categoricas

-------------------------
SISTEMAS DE RECOMENDACION
-------------------------
herramienta que sugiere productos, servicios e informacion a los usuarios en funcion de los datos
conexiones: item-item, usuario-item, usuario-usuario
recomendadores basados en contenido: sugiere recomendaciones basadas en el perfil del usuario, y es mas preciso mientras mas info proporcionen los usuarios
	se basa en la similitud de los elementos recomendados (atributos, categorias)
recomendadores basados en colaboracion:
	enfoque basado en memoria: se basa en las conexiones usuario-usuario e item-item. Se recoge info sobre ellos para encontrar usuarios o articulos similares
	enfoque basado en modelo: se basa en la conexion usuario-item. Se construye el modelo basado en interacciones observadas entre el usuario y el elemento
tambien existen recomendadores hibridos, basados en contenido y colaboracion

#MEDIDAS DE SIMILITUD O DISTANCIA
formas de valoracion mas comunes:
	valoracion binaria: me gusta/no me gusta, 0 o 1
	valoracion numerica: escala tipo ranking, por ejemplo del 1 al 5
	valoracion cantidad: representan cantidad, por ejemplo numero de reproducciones o visitas

distancia euclidiana: distancia entre dos puntos definida como la longitud del segmento que une ambos puntos
distancia de manhattan: distancia entre dos puntos como la sumatoria de las diferencias absolutas entre cada dimension (se ve menos afectada por outliers)
similitud basada en coseno: se mide calculando el coseno del angulo formado por la interseccion de las representaciones vectoriales (util en textos)
similitud basada en correlaciones: se utiliza cuando la similitud es en base a patrones o formas y no desplazamiento o magnitud
	correlacion de pearson: medida de dependencia lineal entre dos variables cuantitativas o cualitativas
	correlacion jackknife: calcula todos los posibles coeficientes de correlacion excluyendo cada vez una de las observaciones
		el promedio de todas las correlaciones atenua el efecto outlier (corrige sesgo de estimacion)
simple matching coefficient (smc): se usa para medir similitud con observaciones con atributos binarios. Nro de coincidencias/nro total de atributos
indice jaccard: equivalente al smc, pero este solo cuenta como coincidencias cuando el atributo esta presente en ambos sets
recomendadores basados en contenido:
	para que funcione se necesita de un perfil de valoraciones del usuario y una cuantificacion de las caracteristicas de productos disponibles

-------------------------------------
APLICACIONES DE IA - REDES NEURONALES
-------------------------------------
la IA se centra en la creacion de programas y mecanismos que muestran comportamientos considerados inteligentes
	- analiza datos en grandes cantidades y alta complejidad
	- identificar patrones y tendencias
	- formular predicciones de forma automatica, con rapidez y precision
las redes neuronales requieren muchos contextos diferentes para funcionar
muy propenso al sobreentrenamiento (aprender de memoria el set de entrenamiento), por eso requiere demasiados datos
las neuronas son la unidad estructural y funcional del sistema nervioso, se especializan en la recepcion de estimulos y conduccion de impulso nervioso
	recibe el mensaje de las dendritas y transmite una señal por medio de un impulso electrico llamado potencial de accion
	dispara o manda el impulso por su axon si la excitacion excede su inhibicion por un valor limite, el umbral de la neurona
	el cerebro aprende cambian las fuerzas de las conexiones, añadiendo o eliminando tales conexiones
las redes neuronales artificiales son un metodo de aprendizaje supervisado para problemas de regresion o clasificacion que emulan el funcionamiento del cerebro
	esta formado por un conjunto de nodos (neuronas) conectadas que transmiten señales entre si mediante una funcion produciendo una respuesta que pasa a otra neurona
	la red aprende examinando registros individuales, generando una prediccion y realizando ajustes cuando se equivoca, hasta haber alcanzado criterios de parada
perceptron: nodo que toma la decision de acuerdo a si pasa o no el umbral (cuando se tiene un unico perceptron es lo mismo que una regresion logistica)
el procesamiento de una red neuronal se organiza en capas:
	capa de entrada (campos de entrada)
	capas ocultas (ponderaciones de los campos de entrada)
	capa de salida (unidades que representan los campos de destino)
	

